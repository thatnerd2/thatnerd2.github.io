<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The Metropolis Hastings Algorithm</title>
    <link rel="stylesheet" href="/css/foundation.min.css" />
    <script type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script src="/js/vendor/modernizr.js"></script>
  </head>
  <body>
    <div class="row">
      <div class="large-12 columns">
        <h1>The Metropolis Hastings Algorithm</h1>
        <h2 class="subheader">many draws</h2>
      </div>
    </div>

    <hr>

    <div class = "row">
      <div class="large-8 medium-8 columns">
        <h2>Prerequisites</h2>
        <p>There's a lot of statistical jargon thrown around in here.  I recommend, if you're interested in really understanding what this algorithm buys you, researching what statistical distributions and probability densities are, how to sample from a known distribution, and perhaps even rejection or importance sampling.  Although, without knowing those concepts, you could probably still get by.</p>

        <h2>Background</h2>
        <p>Let's suppose you have a distribution from which you want to sample.  Unfortunately, there is no easy way to find its density analytically, since sometimes the normalizing constant is really hard to calculate.  However, we can use a technique called Markov Chain Monte Carlo sampling to get something close.  Metropolis-Hastings is one such technique.</p>

        <p>A markov chain is simply a list of numbers, where the number at index i is <b>only dependent on the number at index i - 1</b>.  This means we have a list of slightly dependent draws.  The "slightly dependent" part is the tradeoff for getting what we want - a set of samples from the target distribution (so called <b>kernel/target distribution</b>).  But it turns out that the markov chain we're talking about is <b>ergodic</b>.  Being ergodic means that you satisfy three properties - you're <b>aperiodic</b> (there's no cycle), <b>irreducible</b> (there's a way to go from one state to any other state in the system, albeit it might take more than 1 step), and <b>positive recurrent</b>, so it's expected that the return time to some state \(i\) is finite (a.k.a, you can come back to states within reasonable time).  Most of the Markov chains used in Bayesian Statistics are ergodic.</p>

        <p>Being ergodic buys us the ability to <b>ignore the slight dependence between draws</b>.  More specifically, the Ergodic Theorem says that for an ergodic Markov chain with expected value less than infinity, with probability 1 or absolute certainty,</p> $$\frac{1}{M}\Sigma_{i=1}^Mg(\theta_i) \rightarrow \int_{\Theta}g(\theta)\pi(\theta)d\theta$$

        <p>Each symbol means something.  \(M\) is the number of values in the Markov chain, \(\pi\) is the target distribution we want draws from, \(\theta\) represents the values of the Markov chain, such that \(\theta_1, \theta_2\) refer to the first and second values of the chain, and \(g\), I believe, refers to the probability density of a particular \(\theta\).  Write me a note if you'd like to clarify.</p>

        <p>All this means is that we're good to go mathematically if we draw many Markov chain values from our target distribution.</p>

        <h2>Algorithm</h2>
        <p>There are some ingredients to this algorithm.  We need a chain of values with <b>some arbitrary starting value</b>, and a way to dictate what the next value in the chain will be.  What dictates the next value is called the <b>candidate distribution</b>.  It can be anything, really, but most often people pick symmetric distributions as the candidate distribution.  If the main mass of the target distribution is grouped in some asymmetric way, using an asymmetric candidate distribution might help performance, but this is is a rarer case.  For simplicity's sake, and because hand-tuning the asymmetric candidate distribution is hard, we'll use a very simple candidate distribution - a uniform distribution between x - 3 and x + 3.  x will be the current value of the markov chain.  Notice that this is where we see that the draws are slightly dependent.</p>

        <p>For our <b>target distribution</b>, we'll use a modified T distribution that would ordinarily be hard to find the normalizing constant for.  We'll say that the target distribution is \(\frac{1 + (x - 10)^2}{3})^{-2}\)</p>

        <p>Once we draw a candidate value from the candidate distribution, there's <b>a probability of accepting that value as the next value</b>, which is \(\frac{\text{Target Distribution Density at new value}}{\text{Target Distribution Density at previous value}}\).  This tells us how likely it is to get to that next value, and we generate a random number between 0 and 1 to determine if we actually jump there or not.  If our random probability is less than the acceptance probability, we take the draw.  If not, we keep the old value.  <b>This is a critical feature of the Metropolis-Hastings algorithm.</b></p>

        <p>Intuitively, what's going on is that we're placing a little point somewhere, arbitrarily onto the space.  From that point, we make little jumps in some directions, deciding if it's worthwhile to make the jump.  After many, many jumps, with the probability of each jump dictated by the target distribution, the density of all the jumps/draws we did will be approximately the target distribution we were trying to find.</p>

        <p>"But wait", you say, "how could we possibly be sampling from the distribution if the first point is arbitrary?"  One popular thing to do is to "burn out" the first 100 draws or so, since we know they could be arbitrary.  After taking a million draws, the first 100 are inconsequential.

        <p>Here's some R code for you to solidify the concept.</p>

        <pre>
ALPHA = 3
mh = function (n, target, STARTING_VALUE = 3, burnIn = 10, candidate = function (x) { runif(1, x - ALPHA, x + ALPHA) }) {
  xs = rep(STARTING_VALUE, n)
  for (i in 2:n) {
    y = candidate(xs[i - 1])
    acceptance = target(y) / target(xs[i - 1])
    xs[i] = if (runif(1) < acceptance) y else xs[i - 1]
  }
  xs[burnIn:length(xs)]
}

# Example call to draw from our target distribution
draws = mh(n = 100000, target = function (x) { return(((1 + (x - 10)^2)/3)^(-2)) }, burnIn = 100)
        </pre>

        <p>Notice that <b>the acceptance percentage depends on the spread of the candidate distribution</b>.  For this particular example, I picked a candidate distribution that can select any x value 3 away from the current value.  This yields an acceptance percentage of about 37%.  There's no right answer for the acceptance percentage, but generally I would say that less than 20% is too inefficient and greater than 50% is too relaxed.  Too strict or loose of an acceptance policy could mean, for example, that the most concentrated part of the target distribution will be blown out of proportion by our MCMC algorithm.</p>

        <p>Results (with trace plot)</p>
        <img src="/img/mh-results.png"></img>

        <p>Empirical mean and standard deviation were 9.991816, and 0.8878 respectively.  Quantiles:</p>
        <pre>
  2.5%    25%    50%    75%  97.5% 
 8.247  9.527  9.983 10.424 11.721 
        </pre>

        <p>Notice that the dotted red and black lines are essentially coincidental.  This is good news.  The trace plot shows that the draws were pretty consistent and the MCMC algorithm was doing its job.  A sectioned trace plot, were there are groups of draws in far away places, would be bad.  Also note that 100 entries were burned off the chain.  Another way to reduce the length of the chain is the "thin it out" by removing every nth draw.  I didn't do that here.</p>

        <h2>Diagnostics & Reports</h2>
        <p>When using the MH algorithm, it's important to make sure you and others know:</p>
        <ul>
            <li>Target distribution</li>
            <li>Candidate distribution</li>
            <li>Starting value</li>
            <li># burned off</li>
            <li>Thinning rate (3 means every third entry was removed)</li>
            <li>Empirical mean and standard deviation</li>
            <li>Trace plot</li>
        </ul>

        <p>There are other good convergence and diagnostic statistics, but I think this is a good start.</p>

        <h2>Questions & Answers</h2>
        <p><b>1.  What if I have a starting value outside my target distribution?</b></p>
        <p>Hand-tune your algorithm so that the startig value is not outside your target distribution.  It turns out that most questions I had when I started learning how to do this had to do with specific values and random events.  However, it turns out that it's common practice to hand-tune your Metropolis-Hastings algorithm.  There are adaptive MH algorithms, but often they're cost more than they're worth.</p>

        <p><b>2.  Why not rejection sampling?</b></p>
        <p>Metropolis Hastings is faster, especially for oddly shaped distributions.  If the distribution you'd like to sample from is uniform or fits under a known distribution nicely, then sure, use rejection/importance sampling.</p>
      
        <p><a href="/index.html" class="small button">Home</a><br/>
      </div>

    </div>

    <script src="/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
    </script>
  </body>
</html>
