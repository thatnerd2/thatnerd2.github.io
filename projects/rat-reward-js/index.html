<!DOCTYPE html>

<html>

<head>
	<script type="text/javascript" src="js/phaser.min.js"></script>
	<script type="text/javascript" src="blob.js"></script>
	<link rel="stylesheet" href="../css/foundation.css" />
	<script type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
</head>

<body>
	<div class="row">
      <div class="large-12 columns">
        <h1>Simple Q Reinforcement Learning</h1>
        <h2 class="subheader">neat</h2>
      </div>
    </div>
	<div class = "row">
      <div class="large-8 medium-8 columns">
      	<p>The little green blob right there is a tiny, super-simple artificial intelligence.  Examine the page source and "blob.js" for his Q learning algorithm.  He moves around however he wants.  If he touches the green square, he is rewarded with 50 points.  If he touches the red square, he is punished with -50 points.  Otherwise, he doesn't initially know the layout of the environment, and he doesn't do any planning beforehand.  He stumbles around, learning as he goes, until he figures out that green is good and races toward it all the time (you'll have to let it run for a little while, or adjust the speed).</p>

      	<p>Set Speed (caps at 3000)</p>
      	<input id="speedInput" type="text"></input>
      	<button id="setSpeed">Set</button>

      	<p>Click here to choose a block, and then click somewhere on the stage to set it.</p>
      	<img id="green" width=50 height=50 src="assets/good-space.png"></img>
      	<img id="red" width=50 height=50 src="assets/bad-space.png"></img>
      	<img id="wall" width=50 height=50 src="assets/wall.png"></img>
      	<p id="blockSelectedTxt"></p>
      	<button id="resetBlob">Reset Brain</button>
      	<button id="resetMap">Reset Map</button>
      	<p id="news"></p>
      </div>
 	</div>

 	<div class="row">
		<div id="simulation"></div>
		<div class="large-8 medium-8 columns">
			<h3>Info</h3>
			<p id="numgreen"># of green blocks touched: 0</p>
			<p id="numred"># of red blocks touched: 0</p>
			<button id="clearStats">Clear</button>
			<br /><br />
			<h2>How it works</h2>
			<p>This is about as simple a reinforcement q learning algorithm you can get.  The agent (green blob) stores his position (his "state"), and a set of actions that he can take for that given position.  When he makes an action, he stores a result into a "Q" matrix, which is a map of state, action pairs to "Q" values.</p>

			<p>Let's think about a concrete example.  Green blob is at the top right, right next to that red block.  He can either go up, down, left, or right.  If he decides to move into the red block, his Q value for that state, action pair (e.g. top right, move into the red block) is updated with a -50 punishment.  He won't be so quick to move there next time.</p>

			<p>So given a state and a list of actions, our green blob has a bunch of Q values.  He likes to pick the action that maximizes the Q value of a particular pair.  However, he also likes to explore the environment, in case he finds something cool (maybe a +100 reward?  Too bad they don't exist.)  That's why you'll see him not immediately go for the green every time - he's looking around.  This way he can keep a fairly up-to-date model of the world.  Note that if you place a green block right next to his starting position, he will probably immediately go there every time.  This is because he has no real need to explore - that green block next to him appears to be the best option every time.</p>

			<h3>Calculating Q Values</h3>
			<p>The agent has a few parameters for its brain model, \(\alpha, \gamma, \epsilon\).  In basic terms, \(\alpha\) controls the amount which the rewards affect the Q value updates (how sensitive the agent is to rewards and punishments), \(\gamma\) controls how sensitive the agent is to the rewards/punishments from <b>future actions</b> from the chosen state (the agent knows that if he steps 1 block to the right, he only needs 1 more step up to get the reward), and \(\epsilon\) controls how often the agent chooses to explore rather than maximize the reward of the world he already knows.</p>

			<p>Thus, the update equation goes like this:</p>

			$$
			\begin{align*}
			\text{Let } I &= \text{Reward(current state)}\\
			\text{Let } M &= \text{max(reward of next possible actions)}\\
			\text{Q value for (current state, action)} &= \alpha * (I + \gamma*M - \text{Q value for (current state, action)})
			\end{align*}
			$$

			<p>All Q values are initialized at 0.</p>

			<h3>Why do I care?</h3>
			<p>The green blob is cutesy and all, but why does anyone care?  It turns out that this kind of learning can be applied to lots of different situations where "trying a lot" is not considered bad.  For example, you could have an reinforcement learner who plays tic-tac-toe, and is punished when it loses and rewarded when it wins.  Over many games, it will learn how to play tic tac toe pretty well.</p>

			<p>Also, if you find this interesting, check out "Deep Q Learning", which essentially combines a neural network with a reinforcement learning agent, so that the agent can navigate continuous spaces (where there aren't finite actions/states).  The neural net serves to locate patterns and estimate cost functions and Q values.  It's really neat stuff.</p>

			<p>Feel free to contact me with any questions/comments you have.</p>
		</div>
	</div>
	<script>
		var BLOCK_WIDTH = 50;
		var BLOCK_HEIGHT = 50;
		var blob;
		var objs = [];
		var numGreen = 0;
		var numRed = 0;
		var map = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
				   [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0],
				   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
				   [0, 0, 0, 0, 1, 0, 0, 0, 9, 0, 0, 0],
				   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
				   [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0],
				   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]];
		
		var simState = function (sim) {};

		simState.prototype = {
			preload: function () {
				sim.stage.backgroundColor = 0xffffff;
				sim.load.image('blob', 'assets/blob.png');
				sim.load.image('good', 'assets/good-space.png');
				sim.load.image('bad', 'assets/bad-space.png');
				sim.load.image('wall', 'assets/wall.png');
				sim.physics.startSystem(Phaser.Physics.ARCADE);
			},

			create: function () {
				var blobr = 0;
				var blobc = 0;
				for (var r = 0; r < map.length; r++) {
					for (var c = 0; c < map[r].length; c++) {
						if (map[r][c] == 1) this.build(r, c, 'wall');
						else if (map[r][c] == 2) this.build(r, c, 'good');
						else if (map[r][c] == 3) this.build(r, c, 'bad'); 
						else if (map[r][c] == 9) {
							blobr = r;
							blobc = c;
							map[r][c] = 0;
						}
					}
				}

				blob = new Blob(sim, blobr, blobc);
				objs.push(blob);

				var actionLoop = function () {
					var nextBlobPos = blob.act();
					var tween = sim.add.tween(blob.spr).to({
						x: nextBlobPos[1] * BLOCK_WIDTH,
						y: nextBlobPos[0] * BLOCK_HEIGHT
					}, blob.speed);
					tween.onComplete.add(function () {
						if (map[nextBlobPos[0]][nextBlobPos[1]] != 0) {
							if (map[nextBlobPos[0]][nextBlobPos[1]] == 2) {
								numGreen += 1;
								$("#numgreen").html("# of green blocks touched: " + numGreen);
							} else {
								numRed += 1;
								$("#numred").html("# of red blocks touched: " + numRed);
							}

							blob.setPosition(blobr, blobc);
						}
						actionLoop();
					});
					tween.start();
				}
				actionLoop();

				sim.input.onDown.add(function () {
					if (selectedBlock == "") return;

					var c = parseInt(sim.input.activePointer.x/parseFloat(BLOCK_WIDTH));
					var r = parseInt(sim.input.activePointer.y/parseFloat(BLOCK_HEIGHT));
					if (r < 0 || r >= map.length || c < 0 || c >= map[r].length) return;

					map[r][c] = selectedBlock;
					var assetName = (selectedBlock == 1) ? "wall" : ((selectedBlock == 2) ? "good" : "bad");
					this.build(r, c, assetName);
			  	}.bind(this));
			},

			build: function (r, c, asset) {
				var thing = sim.add.sprite(c * BLOCK_WIDTH, r * BLOCK_HEIGHT, asset);
				thing.width = BLOCK_WIDTH;
				thing.height = BLOCK_HEIGHT;
				objs.push(thing);
			}
		}

		var sim = new Phaser.Game(600, 400, Phaser.AUTO, 'simulation');
		sim.state.add('sim-state', simState);
		sim.state.start('sim-state');


		function isValidBoardPosition (pos) {
			if (pos[0] < 0 || pos[0] >= map.length || pos[1] < 0 || pos[1] >= map[0].length) return false;
			if (map[pos[0]][pos[1]] == 1) return false;
			return true;
		}

		function getReward (pos) {
			if (map[pos[0]][pos[1]] == 2) return 50;
			else if (map[pos[0]][pos[1]] == 3) return -50;
			return 0;
		}

		var selectedBlock = "";


		$("#green").click(function () {
			selectedBlock = 2;
			$("#blockSelectedTxt").html("Green block selected (+50)");
		});

		$("#red").click(function() {
			selectedBlock = 3;
			$("#blockSelectedTxt").html("Red block selected (-50)");
		});

		$("#wall").click(function () {
			selectedBlock = 1;
			$("#blockSelectedTxt").html("Wall block selected");
		});

		$("#setSpeed").click(function () {
			var input = $("#speedInput").val();
			if (!isNaN(input)) {
				var newSpeed = parseFloat(input);
				blob.speed = Math.max(3000 - input, 10);
			}
		});

		$("#resetBlob").click(function () { 
			$("#news").html("Brain has been reset."); 
			blob.resetBrain(); 
		});

		$("#clearStats").click(function () {
			numGreen = 0;
			numRed = 0;
			$("#numgreen").html("# of green blocks touched: " + numGreen);
			$("#numred").html("# of red blocks touched: " + numRed);
		});

		$("#resetMap").click(function () {
			map = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
				   [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0],
				   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
				   [0, 0, 0, 0, 1, 0, 0, 0, 9, 0, 0, 0],
				   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
				   [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0],
				   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]];
			sim.state.start("sim-state", true, false);
		});
	</script>
	
</body>